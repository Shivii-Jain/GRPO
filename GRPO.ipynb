{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivii-Jain/GRPO/blob/main/GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "bc196bcfc17344ce89d7827e3fa92c41",
            "71ae31bef7a044e8877fe6deb7974b63",
            "885fa6d4161b489ab1a23c0e051175f6",
            "b74285951f2047589446644d56b6fd74",
            "041f78ea21c34b3d9dd2860283703012",
            "1c86308382ec49a7b8c9da0c22595d32",
            "43f4e8a2b42543fb9e4eecd259b0910c",
            "f07425fa46724e718a2f816e47dcfffa",
            "ba9f903982f54b13911a02c0a21a53d5",
            "7d88be00cf3b4d3eb79ce241c9e961d4",
            "5ebf7b22702e47de916dcfb7cd44679f",
            "02e4fb824ac741a9973c1c63e2d8117c",
            "37399c28185942858de2573c5075a558",
            "cd79cb3a8c4b41f692d8a57b254b6231",
            "3065d9aa6e41489b8f2b267ce3eaabcd",
            "2511030c94f040cdbe18421d4d9e3426",
            "d4d4f865747748559879d4e93c5bb7a2",
            "13012a7bb74a401e85468983cd4de195",
            "a2eb6deb2c474592819721a8163ec34f",
            "07c285f6e0c34d11b6a1f34c2460dc29",
            "40bca2a262494d8997cc1fea28665485",
            "225d153da83943388d00ca2b700749b4",
            "5248e759f3024bdb8b7143fb4162905f",
            "e7fb0cb64af543e8b42bb515fa2e0dc2",
            "af8e3bea14a14ba5b021200b6ba8e595",
            "5824212eaa024f8c895f615e1beea7f8",
            "8f0013d43d5247e38bf1ae33f905ee1c",
            "d85a419ed9db44d8a4c9a4a8845699e8",
            "ddbd73c69fd64cdbbae3bd075c57f3c1",
            "3da8c0475005420db70e74247f1222e2",
            "c1c2b73949f949609917bb8308c0e7ae",
            "e8619152de084b9497ae5750d2da3c2e",
            "89495228dfc045d9992b7306b433a327",
            "de2cc4303fc6485299d1bfacb19305ff",
            "1b8d751ed6e345f98663bcb7d83d002f",
            "514ab761f14441428e7a61b3d40d9b28",
            "cb833e00d8e6431b9f1482e0d5d0c15d",
            "c9427cfbc9cf4ed9b287b82d93df858b",
            "7d929a585b3a4628b714d6280a40cfaf",
            "bc1266fc8c244dc88868cbdafbb92414",
            "333fe62f6c3f41799110f926a1c50259",
            "a63b4beb8bb748fc92ef47950cdc6e98",
            "6b7d2b43a63d41a7bacf8bff8fd9a6dc",
            "61a32f12dc89455a93b68ce487efae78",
            "decc8c33926d4883b45d6a72bfd0dfff",
            "b0b53a9ae0cc4536a3a6de7db4921aed",
            "9c3858ef34f94962a61fe1f1024c36b9",
            "65f92f4cb3614f5a8fd2aeb781c83b10",
            "9f1c81352a7e4cb8a65a961ff045946f",
            "c98bf32f5d004a1ba0f0b41acfd724eb",
            "28aa5d54c12341b28f7772e085625a2f",
            "2ab48aacd9ae4dfebfeebc56a21520b4",
            "d872ea315e8b454589dd9e8804f432c7",
            "3a42052f88c94e5791e02f6a66927818",
            "a73d149e67034e0fa7d02e63ffdfd233",
            "55585003c2614290ad9fae8011e062f1",
            "f47c78158fb04f099dab7a4ca71ff5f3",
            "de8c91d75f5541ba842c9db1d4ba18be",
            "49ae5ba203bb48ff849c2edae3f5f4a0",
            "8477c796161842b7948889fcfe8ba945",
            "6a1549fce08c4ae5bfa1b5659aa9da6a",
            "fb64db1bfbb24366b4bf9627b5d7eee3",
            "7ca74815643746d9a0832a11c8fdf7cd",
            "68a0a87d7a8f4da7aed8635f4f58e847",
            "9fbbf7e7a5434ad3ba78eae0c0979025",
            "963df056eb1349638deda2fdfb42cfa5",
            "245d0e4b93d34243b3b103ddeed65f7c",
            "f2618828be4b43a3b5d10bc6e920d6c7",
            "8aa77d1087f9456c8fd8a193f67e12cc",
            "b0845af5e67a4306a66a250a0c88762b",
            "24e22a7cd6bc4a96a699cb15e8a2efd1",
            "e8f0a4b74e4445afb00496a4141e4ff1",
            "7a766a39f02146919deb7e71acb7027f",
            "2d005da530f5472ab2e82eb7f8d335d5",
            "c0d80eee02724c1ba6ded47a5532003e",
            "2b89b3341ea04c70a04b3d114ecfb3f8",
            "319aef116ff14c998da67ded55594066",
            "1b5aecb553d3429891bb21d283fc447a",
            "a75e28ddbe95429ea39de132b3ebdf24",
            "bd98feefa8aa43b6a3eaaf1b977b2110",
            "41cc9f18c404404b88119054920a6def",
            "ca77a679c09747808ea735fc1bc99f07",
            "8f1cc7a715eb4419a51b93748941a8a3",
            "8b92954ea55e470bbf6a914c9616db2a",
            "ba769647c05f452c88eed9f0b2aee592",
            "bcbfe230fa764926a4cbdd5f0ee7d03c",
            "59aabe9adbaa4e4696b7f947fc308653",
            "bac81f96039d489585172aeaf3b3a82a"
          ]
        },
        "id": "qm1bdPMhFJOn",
        "outputId": "50f716fc-cb79-4edb-8324-ebbb1b64f487"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc196bcfc17344ce89d7827e3fa92c41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02e4fb824ac741a9973c1c63e2d8117c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5248e759f3024bdb8b7143fb4162905f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de2cc4303fc6485299d1bfacb19305ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "decc8c33926d4883b45d6a72bfd0dfff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55585003c2614290ad9fae8011e062f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "245d0e4b93d34243b3b103ddeed65f7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b5aecb553d3429891bb21d283fc447a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded EleutherAI/gpt-neo-125M with 125.2M parameters\n"
          ]
        }
      ],
      "source": [
        "#Setup and Model Loading\n",
        "\n",
        "# importing libraries\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"EleutherAI/gpt-neo-125M\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"left\"\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# This is trainable policy model\n",
        "policy_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Frozen reference model\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "ref_model.eval()\n",
        "for p in ref_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "n_params = sum(p.numel() for p in policy_model.parameters())\n",
        "print(f\"Loaded {model_name} with {n_params/1e6:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfF-CCznSLaL",
        "outputId": "c7093ff6-725c-42e8-f33e-1cc4667188a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset sizes:\n",
            "Training: 800\n",
            "Evaluation: 100\n",
            "\n",
            "Example prompts:\n",
            "{'prompt': 'ear is to hear as eye is to', 'targets': ['see', 'look', 'watch']}\n",
            "{'prompt': 'word is to sentence as note is to', 'targets': ['melody', 'tune']}\n",
            "{'prompt': 'winter is to cold as summer is to', 'targets': ['hot', 'warm']}\n"
          ]
        }
      ],
      "source": [
        "# Dataset Generation\n",
        "\n",
        "# Base analogy patterns: A, B, C, [list of acceptable D targets(taken from wikipedia)]\n",
        "base_analogies = [\n",
        "    (\"sun\",      \"bright\",   \"moon\",    [\"dim\", \"pale\", \"glowing\", \"luminous\", \"reflective\"]),\n",
        "    (\"king\",     \"man\",      \"queen\",   [\"woman\", \"lady\", \"female\"]),\n",
        "    (\"cat\",      \"kitten\",   \"dog\",     [\"puppy\"]),\n",
        "    (\"teacher\",  \"school\",   \"doctor\",  [\"hospital\", \"clinic\"]),\n",
        "    (\"rain\",     \"wet\",      \"snow\",    [\"cold\", \"white\", \"icy\"]),\n",
        "    (\"fire\",     \"hot\",      \"ice\",     [\"cold\", \"freezing\", \"chilly\"]),\n",
        "    (\"bird\",     \"fly\",      \"fish\",    [\"swim\", \"swimming\"]),\n",
        "    (\"word\",     \"sentence\", \"note\",    [\"melody\", \"tune\"]),\n",
        "    (\"ear\",      \"hear\",     \"eye\",     [\"see\", \"look\", \"watch\"]),\n",
        "    (\"lion\",     \"courage\",  \"fox\",     [\"cunning\", \"clever\", \"sly\"]),\n",
        "    (\"knife\",    \"cut\",      \"pen\",     [\"write\", \"scribble\"]),\n",
        "    (\"car\",      \"road\",     \"boat\",    [\"water\", \"sea\", \"river\"]),\n",
        "    (\"winter\",   \"cold\",     \"summer\",  [\"hot\", \"warm\"]),\n",
        "    (\"seed\",     \"plant\",    \"egg\",     [\"bird\", \"chick\"]),\n",
        "    (\"up\",       \"down\",     \"left\",    [\"right\"]),\n",
        "    (\"strong\",   \"strength\", \"wise\",    [\"wisdom\", \"insight\"]),\n",
        "    (\"mother\",   \"parent\",   \"son\",     [\"child\", \"kid\"]),\n",
        "    (\"glass\",    \"transparent\",\"brick\", [\"opaque\", \"solid\"]),\n",
        "    (\"bee\",      \"honey\",    \"cow\",     [\"milk\"]),\n",
        "    (\"author\",   \"book\",     \"composer\",[\"music\", \"symphony\", \"song\"]),\n",
        "]\n",
        "\n",
        "def build_dataset(repeats: int = 40):\n",
        "    data = []\n",
        "    for i in range(repeats):\n",
        "        for A, B, C, targets in base_analogies:\n",
        "            prompt = f\"{A} is to {B} as {C} is to\"\n",
        "            data.append({\"prompt\": prompt, \"targets\": targets})\n",
        "    random.shuffle(data)\n",
        "    return data\n",
        "\n",
        "train_data = build_dataset(repeats=40)   # appox 800 samples\n",
        "eval_data  = build_dataset(repeats=5)    # approx 100 samples\n",
        "\n",
        "print(\"Dataset sizes:\")\n",
        "print(\"Training:\", len(train_data))\n",
        "print(\"Evaluation:\", len(eval_data))\n",
        "\n",
        "print(\"\\nExample prompts:\")\n",
        "for ex in train_data[:3]:\n",
        "    print(ex)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnHD44gVH53o",
        "outputId": "770162f3-164a-492c-da1e-567bf1bd4af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sanity check:\n",
            "Completion: dim             -> Reward: 1.0\n",
            "Completion: tall            -> Reward: 0.0\n",
            "Completion: luminous        -> Reward: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Reward Function Implementation\n",
        "\n",
        "def extract_first_word(text: str) -> str:\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    tokens = text.split()\n",
        "    if len(tokens) == 0:\n",
        "        return \"\"\n",
        "    w = tokens[0].strip(string.punctuation).lower()\n",
        "    return w\n",
        "\n",
        "def reward(prompts, completions, targets_list):\n",
        "    rewards = []\n",
        "    for comp, targets in zip(completions, targets_list):\n",
        "        can_word = extract_first_word(comp)\n",
        "        targets_lower = [t.lower() for t in targets]\n",
        "        if can_word in targets_lower:\n",
        "            rewards.append(1.0)\n",
        "        else:\n",
        "            rewards.append(0.0)\n",
        "\n",
        "    return torch.tensor(rewards, dtype=torch.float32, device=device)\n",
        "\n",
        "#Sanity Check\n",
        "test_prompt = \"sun is to bright as moon is to\"\n",
        "test_targets = [[\"dim\", \"pale\", \"glowing\", \"luminous\", \"reflective\"]]\n",
        "test_completions = [\"dim\", \"tall\", \"luminous\"]\n",
        "\n",
        "print(\"\\nSanity check:\")\n",
        "for comp in test_completions:\n",
        "    r = reward([test_prompt], [comp], test_targets).item()\n",
        "    print(f\"Completion: {comp:<15} -> Reward: {r:.1f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZN4Gv2xIbah",
        "outputId": "97616eab-0160-4439-db88-72c8f939f8d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advantages shape: torch.Size([4, 4])\n",
            "Initial total loss: -0.9311904311180115\n",
            "Initial avg reward: 0.3125\n"
          ]
        }
      ],
      "source": [
        "# GRPO Implementation\n",
        "\n",
        "G = 4                 #group size\n",
        "beta_kl = 0.1         #KL regularization strength\n",
        "eps = 1e-5\n",
        "\n",
        "def grpo_step(batch, return_details=False):\n",
        "\n",
        "    prompts = [ex[\"prompt\"] for ex in batch]\n",
        "    targets_list = [ex[\"targets\"] for ex in batch]\n",
        "    batch_size = len(prompts)\n",
        "\n",
        "    all_log_probs = []\n",
        "    all_rewards   = []\n",
        "    all_kls       = []\n",
        "    all_completions_groups = []\n",
        "\n",
        "    prompt_enc = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
        "    prompt_input_ids = prompt_enc[\"input_ids\"]\n",
        "    prompt_lens = prompt_input_ids.ne(tokenizer.pad_token_id).sum(dim=1)\n",
        "\n",
        "    for g in range(G):\n",
        "        enc = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            gen_ids = policy_model.generate(\n",
        "                **enc,\n",
        "                max_new_tokens=1,\n",
        "                do_sample=True,\n",
        "                temperature=1.0,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "        full_texts = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
        "        completions = [\n",
        "            full[len(prompt):].strip()\n",
        "            for full, prompt in zip(full_texts, prompts)\n",
        "        ]\n",
        "        all_completions_groups.append(completions)\n",
        "\n",
        "        rewards = reward(prompts, completions, targets_list)\n",
        "        all_rewards.append(rewards)\n",
        "\n",
        "\n",
        "        seq_input = gen_ids[:, :-1]\n",
        "        seq_labels = gen_ids[:, 1:]\n",
        "        attn_mask = (seq_input != tokenizer.pad_token_id).long()\n",
        "\n",
        "        outputs = policy_model(input_ids=seq_input, attention_mask=attn_mask)\n",
        "        logits = outputs.logits\n",
        "        log_probs_all = F.log_softmax(logits, dim=-1)\n",
        "        token_logp = log_probs_all.gather(-1, seq_labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        seq_logps = []\n",
        "        for i in range(batch_size):\n",
        "            pl = prompt_lens[i].item()\n",
        "            lp = token_logp[i, pl-1:].sum()\n",
        "            seq_logps.append(lp)\n",
        "        seq_logps = torch.stack(seq_logps)\n",
        "        all_log_probs.append(seq_logps)\n",
        "\n",
        "        # KL estimate vs reference model\n",
        "        with torch.no_grad():\n",
        "            ref_outputs = ref_model(input_ids=seq_input, attention_mask=attn_mask)\n",
        "            ref_logits = ref_outputs.logits\n",
        "            ref_log_probs_all = F.log_softmax(ref_logits, dim=-1)\n",
        "            ref_token_logp = ref_log_probs_all.gather(-1, seq_labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "            ref_seq_logps = []\n",
        "            for i in range(batch_size):\n",
        "                pl = prompt_lens[i].item()\n",
        "                lp = ref_token_logp[i, pl-1:].sum()\n",
        "                ref_seq_logps.append(lp)\n",
        "            ref_seq_logps = torch.stack(ref_seq_logps)\n",
        "\n",
        "        kl_estimate = (seq_logps - ref_seq_logps)\n",
        "        all_kls.append(kl_estimate)\n",
        "    log_probs = torch.stack(all_log_probs, dim=0)\n",
        "    rewards = torch.stack(all_rewards, dim=0)\n",
        "    kls= torch.stack(all_kls, dim=0)\n",
        "\n",
        "    mean_rewards = rewards.mean(dim=0, keepdim=True)\n",
        "    std_rewards  = rewards.std(dim=0, unbiased=False, keepdim=True) + eps\n",
        "\n",
        "    advantages = (rewards - mean_rewards) / std_rewards\n",
        "\n",
        "    loss_policy = -(advantages * log_probs).mean()\n",
        "    loss_kl = kls.mean()\n",
        "    loss_total = loss_policy + beta_kl * loss_kl\n",
        "\n",
        "    avg_reward = rewards.mean().item()\n",
        "\n",
        "    if return_details:\n",
        "        return loss_total, loss_policy.detach(), loss_kl.detach(), advantages.detach(), avg_reward, all_completions_groups\n",
        "    else:\n",
        "        return loss_total, avg_reward\n",
        "\n",
        "# Testing one GRPO step\n",
        "test_batch = train_data[:4]\n",
        "loss, loss_pol, loss_kl, A, avg_r, comp_groups = grpo_step(test_batch, return_details=True)\n",
        "\n",
        "print(\"Advantages shape:\", A.shape)\n",
        "print(\"Initial total loss:\", loss.item())\n",
        "print(\"Initial avg reward:\", avg_r)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvI0c2U6JQQf",
        "outputId": "55ce46fc-69c5-4612-dad9-0d607ed544af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 020] Loss = -76.3048 -  Avg Reward = 0.562\n",
            "[Step 040] Loss = 0.2952 -  Avg Reward = 0.469\n",
            "[Step 060] Loss = -86.7424 -  Avg Reward = 0.875\n",
            "[Step 080] Loss = -76.7771 -  Avg Reward = 0.875\n",
            "[Step 100] Loss = -69.2336 -  Avg Reward = 0.969\n",
            "[Step 120] Loss = -0.0240 -  Avg Reward = 0.688\n",
            "[Step 140] Loss = 0.4905 -  Avg Reward = 1.000\n",
            "[Step 160] Loss = 0.6562 -  Avg Reward = 0.625\n",
            "[Step 180] Loss = -47.9028 -  Avg Reward = 0.875\n",
            "[Step 200] Loss = 0.5123 -  Avg Reward = 0.594\n",
            "[Step 220] Loss = 0.3055 -  Avg Reward = 0.594\n",
            "[Step 240] Loss = 0.4250 -  Avg Reward = 0.625\n",
            "[Step 260] Loss = 0.4413 -  Avg Reward = 0.781\n",
            "[Step 280] Loss = -173.8563 -  Avg Reward = 0.812\n",
            "[Step 300] Loss = -117.2856 -  Avg Reward = 0.875\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "\n",
        "optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5)\n",
        "\n",
        "steps   = 300\n",
        "batch_size  = 8\n",
        "log_every   = 20\n",
        "\n",
        "policy_model.train()\n",
        "\n",
        "for step in range(1, steps + 1):\n",
        "    batch = random.sample(train_data, batch_size)\n",
        "    loss, avg_r = grpo_step(batch, return_details=False)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if step % log_every == 0:\n",
        "        print(f\"[Step {step:03d}] Loss = {loss.item():.4f} -  Avg Reward = {avg_r:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31oqJN4HJxoX",
        "outputId": "173f80a2-3df1-4014-b711-d7c5af796d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantitative Evaluation\n",
            "Baseline Avg Reward: 0.150\n",
            "GRPO Avg Reward: 0.800\n",
            "Qualitative Examples: \n",
            "\n",
            "Prompt:   rain is to wet as snow is to\n",
            "Targets:  ['cold', 'white', 'icy']\n",
            "Baseline → 'fall' (reward=0.000)\n",
            "Fine-tuned → 'cold' (reward=1.000)\n",
            "\n",
            "Prompt:   knife is to cut as pen is to\n",
            "Targets:  ['write', 'scribble']\n",
            "Baseline → 'cut' (reward=0.000)\n",
            "Fine-tuned → 'write' (reward=1.000)\n",
            "\n",
            "Prompt:   teacher is to school as doctor is to\n",
            "Targets:  ['hospital', 'clinic']\n",
            "Baseline → 'school' (reward=0.000)\n",
            "Fine-tuned → 'clinic' (reward=1.000)\n",
            "\n",
            "Prompt:   ear is to hear as eye is to\n",
            "Targets:  ['see', 'look', 'watch']\n",
            "Baseline → 'see' (reward=1.000)\n",
            "Fine-tuned → 'see' (reward=1.000)\n",
            "\n",
            "Prompt:   word is to sentence as note is to\n",
            "Targets:  ['melody', 'tune']\n",
            "Baseline → 'sentence' (reward=0.000)\n",
            "Fine-tuned → 'phasis' (reward=0.000)\n"
          ]
        }
      ],
      "source": [
        "# Evaluation and Generation\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, dataset, num_samples=100):\n",
        "    model.eval()\n",
        "    sample = random.sample(dataset, min(num_samples, len(dataset)))\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for ex in sample:\n",
        "        prompt = ex[\"prompt\"]\n",
        "        targets = ex[\"targets\"]\n",
        "\n",
        "        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        gen_ids = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "        full = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "        completion = full[len(prompt):].strip()\n",
        "\n",
        "        r = reward([prompt], [completion], [targets]).item()\n",
        "        total_reward += r\n",
        "\n",
        "    avg = total_reward / len(sample)\n",
        "    model.train()\n",
        "    return avg\n",
        "\n",
        "#Quantitative Evaluation\n",
        "baseline_reward = eval_model(ref_model,  eval_data, num_samples=100)\n",
        "finetuned_reward = eval_model(policy_model, eval_data, num_samples=100)\n",
        "\n",
        "print(\"Quantitative Evaluation\")\n",
        "print(f\"Baseline Avg Reward: {baseline_reward:.3f}\")\n",
        "print(f\"GRPO Avg Reward: {finetuned_reward:.3f}\")\n",
        "\n",
        "#Qualitative Examples\n",
        "print(\"Qualitative Examples: \")\n",
        "for i in range(5):\n",
        "    ex = eval_data[i]\n",
        "    prompt = ex[\"prompt\"]\n",
        "    targets = ex[\"targets\"]\n",
        "\n",
        "    #Baseline\n",
        "    with torch.no_grad():\n",
        "        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        base_ids = ref_model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "        base_full = tokenizer.decode(base_ids[0], skip_special_tokens=True)\n",
        "        base_comp = base_full[len(prompt):].strip()\n",
        "        base_reward = reward([prompt], [base_comp], [targets]).item()\n",
        "\n",
        "    #fine-tuning\n",
        "    with torch.no_grad():\n",
        "        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        ft_ids = policy_model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "        ft_full = tokenizer.decode(ft_ids[0], skip_special_tokens=True)\n",
        "        ft_comp = ft_full[len(prompt):].strip()\n",
        "        ft_reward = reward([prompt], [ft_comp], [targets]).item()\n",
        "\n",
        "    print(f\"\\nPrompt:   {prompt}\")\n",
        "    print(f\"Targets:  {targets}\")\n",
        "    print(f\"Baseline → '{base_comp}' (reward={base_reward:.3f})\")\n",
        "    print(f\"Fine-tuned → '{ft_comp}' (reward={ft_reward:.3f})\")\n"
      ]
    }
    ],
    "metadata": {
      "accelerator": "GPU",
      "colab": {
        "gpuType": "T4",
        "provenance": [],
        "authorship_tag": "ABX9TyM3rDs6+Z/y9OikLE0QneiE",
        "include_colab_link": true
      },
      "kernelspec": {
        "display_name": "Python 3",
        "name": "python3"
      },
      "language_info": {
        "name": "python"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 0
  }
